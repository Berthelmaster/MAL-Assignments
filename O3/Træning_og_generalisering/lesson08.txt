ITMAL NOTE til Slides:
LESSON 8: Model-capacity, Under- and Overfitting, Generalization

-----------------------------------------------------------------------------

Slide 2 til 7:

Real-world systemer, der anvender ML.

(skulle have været en klasse diskussion).

-----------------------------------------------------------------------------

Slide 2: Shimon

En ML-robot, der spiller musik. Var forbi Aarhus Musikhus i efteråret.

'Kreativ' robot idet fejl (FP og FN), leder til alternative fortolkninger af
musikken...den spiller jo Jazz! Herved giver False-Positive og False-Negative
direkte værdi i stedet for at give problemer!

Robotten ledsages af omkring 5-6 ingeniører og en professor, så den tager ikke
arbejdet fra os!

ML-system Spørgsmål: hvornår rejser Shimon selv rundt, checker ind og ud af
lufthavn og sørger selv for strøm og instrumenter...uden at syv andre
ingeniører løber efter ham?

-----------------------------------------------------------------------------

Slide 3: Busser

Nyligt indsatte busser i Aalborg Øst, dansk firma, teknologi ikke kendt og er
ikke at finde på hjemmesiden.

Kører 18 til 25 km/t og der sidder stadig en chauffør i bussen med et
joystick og nødstop (i test fasen).

ML-system Spørgsmål: Hvornår kører den 80 km/t og uden chauffør, og ikke i et
lukket system men på de offentlige veje?

ML-system Spørgsmål: Findes der ikke andre selvkørende biler, der er 100% i
drift på det offentlige vejnet?

-----------------------------------------------------------------------------

Slide 5: På roadtrip med en insekthjerne

Brug af ML til at lave beat-poesi, igen giver FP og FN direkte (positiv) værdi,
som kreativt indspark fra en "insekthjerne" (ML system).

Når ML er nået til Weekendavisen (kræver helst at læseren ryger pibe eller er over
45 år, helst begge dele), er vi så ikke nået toppen af "Expectation-Time"
kurven, og står over for at ryge ned i "Trought of Disillusionment"?

Se også referencen til "AlphaGo", som vi snakkede om i lektion 1.

-----------------------------------------------------------------------------

Slide 5+6: Detektering af mide på bier

System lavet her på ASE til at finde mider på bier. Se en mide i den røde
firkant på Slide 4 (den er ikke ret tydelig).

Brug af ML indgår som et neuralt-netværk (CNN), og er kun en meget lille del af
den totale billede-processerings-pipleline (Se pipeline, Slide 5, se CNN i hjørnet
af "Varroa classification and localization" block).

Dvs. at dette system ikke direkte kunne bruge ML til at finde mider, men kun
benytte ML (CNN) som en lille sub-system til at løse hele problem-komplekset.

Krævede mange ingeniører og en del udviklings-år til konstruktion..igen nemme
ML løsninger direkte, øv!

ML-system Spørgsmål: hvorfor kan ML ikke bare løse dette problem af sig selv?

Filosofisk ML Spørgsmål: Tror du at vi når "Singularity" i år 2045, dvs.
tidspunktet, hvor human- og machine-intellicence har et break-even, og
hvorefter maskinerne, med en eksponentiel udvikling, er bedre en menneskerne
(slides fra L01)?

OK,  måske/måske-ikke i år 2045, men tror du at ML vil kunne overgå menneskelig
intelligens overhovedet?

-----------------------------------------------------------------------------

Slide 7: kløver mark med insekter

BA projekt fremlægges, generelt Tagging/labeling tool til at finde insekter!

Problemstilling: lav en system, der kan finde insekter i billeder ala denne
kløvermark. Billeddata er i TerraByte størrelsen! Baggrund og belysning
forskellig (nat/dag, regn/sol, skygge + vand refleksioner osv.).

For at kunne lave dette skal der laves et datagrundlag til Supervised
learning...og hermed et værktøj, så vi kan sidde og manuelt finde insekter.

BA projektet går ud på at lave et sådanne generelt labeling værktøj. Ellers
frie hænder mht. GUI (QT, Web) men systemet skal kunne kører på både Linux og
Windows...

-----------------------------------------------------------------------------

Slide 8: The Map

I gang med dagens emne: træning og generalisering.  Dvs.  at vi kigger på hvad
der sker når vi har trænet og går i gang med at se på nye data: ind kommer
X^{new} som så bliver outputtet som y^{new} i inference/run blokken (grøn
tekst).

Dette viser, hvordan ML systemet er i stand til at generalisere (grøn kasse i
midten!).

Husk på at vores dummy binary classifier til MNIST havde en accuracy på ca. 90%,
men ikke besad nogen form for generaliserings evne!  Dvs.  at den i
ML-sammenhæng er ligegyldig...et ML system skal kunne forudsige på nye data med
en hvis kvalitet.

-----------------------------------------------------------------------------

Slide 9: Pipelines

Kort intro til brug af pipelines i Python. Når i laver jeres eget system, vil i
skulle pre-processere jeres data igen-og-igen.

I en pipeline loader man derfor de rå data, og introducere så flere og flere
blokke, der modificerer eller reducerer data inden selve træning går
i gang...det minder lidt om min billedbehandlings-pipeline på Slide 5, for selve
"Low-level processing" pipelinen.

Det essentielle i Pipelinen er (lidt som i en testsuite) at man kan
automatisere processen, og med pipeline-koden har en "formel" beskrivelse af,
hvad man gør med data, i stedet for at det bliver en manuel process, der er svær
eller umulig at genskabe.

Pipelines i Python følger fit (predict/transform?) interfacet, og husk at nye
data skal preprocesseres på samme måde som jeres test data, dvs. kører igennem
samme pipeline som under træning.

-----------------------------------------------------------------------------

Slide 10: Resume, performance_metrics.ipynb

Vi hopper kort tilbage til loss funktioner og scores/performace metrikkerne,
som i nu har styr på.

Kørte i Keras MLP opgaven fra sidste lektion, så i at loss funktionen
"categorical_crossentropy" blev brugt til træning, selvom at vi primært har
behandlet loss funktionerne MSE (evt. RMSE), og MAE.

En meget brugt loss funktion er "crossentropy" (som også kaldes log
loss/logistic).  Den har vi ikke behandlet, men detaljer kan findes i
Scikit-learn (hvis du har mod):

  https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html?highlight=cross%20entropy

I Scikit-learn finder i også, som sædvanligt, en hel menu af loss-funktioner
(kaldet Regression metrics i slide), og grænsen mellem en loss- og en score-
funktion udviskes gradvist. Generelt er en høj værdi for en score funktion at
foretrække (jo højere score jo bedre), mens en lav loss er bedst (jo lavere
loss værdi jo bedre). Derfor kunne man f.eks. lave en score der var lig -MSE
(dvs. minus mean-square-error loss'en).

Tidligere brugte bla.  accuracy som score (der ikke er en rigtig
metrik/afstandsmål). Den hedder i Scikit-learn "metrics.accuracy_score"
og i Keras "categorical_accuracy".

Bemærk her at Keras' dokumentation er meget mangelfuld i forhold til
Scikit-learn. Til gengæld er performance af Keras (+TensorFlow) i top, mens
performance af Scikit-learn er i bund: god performance er ikke et design princip
i Scikit-learn, god dokumentation er ikke et design princip for Keras.

Endelig viste Keras MLP-opgaven at man kunne opsamle og gemme diverse metrikker
undervejs, her er f.eks. "categorical_accuracy", "MSE" og "MAE" gemt i en
historik, som så kan plottes til sidst. Under træning kan der gå lang tid før
at MSE/MAE går ned eller at accuracy går op, fordi 'systemet' befinder sig på
et plateau, hvor det tager lang tid før det når ned i et minimum.

Ved tuning af model-hyperparametre, kan man få 'systemet' til hurtigere at
iterere sig ned mod et minimum...det kommer vi ind på under "Søgnings"-lektionen
næste gang.

-----------------------------------------------------------------------------

Slide 11: Model capacity

Et ML-system skulle kunne generalisere. Dummy binær classifieren havde en
"model kapacitet" på nul, og kunne ikke generalisere.

For et polynomisk fit kan man ca. sige at modelkapaciteten er "lig"  graden af
polynomiet, dvs. jo højere polynomiumsgrad jo større kapacitet.

For et NN, er det sværere direkte at sige noget om model kapaciteten, men den
skalerer på en måde med antallet af neuroner, jo flere neuroner, jo større
model kapacitet.

Nu kan man så skrue kapaciteten op så meget man vil og træningsfejlen vil gå
mod nul, men det gør så ikke noget godt for systemets evne til at generalisere,
for hvad sker der for et system med al for stor model-kapacitet, når vi kører
på nye data (X^{new} i The Map)?

Og hvordan vælger du den "optimale" model kapacitet?

-----------------------------------------------------------------------------

Slide 12: Under- and overfitting

Opgave til aflevering: "capcity_under_overfitting.ipynb", som viser de to
koncepter: under- og overfitting af data.

Vi har snakket kort om under- og overfitting, men opgaven her går ud på at se
helt ned i detaljerne. En Cosinus funktion med støj forsøges at blive fittet
med en Polynomiums-model af degree=1, 4 og 15.

De forskellige degree's leder så til hhv. under og overfitting, fordi
model-kapaciteten er hhv. for lille og for stor.

Men hvordan vælges så den optimale model kapacitet...det mangler vi stadig at
kunne svare på!?

-----------------------------------------------------------------------------

Slide 13+14: Generalizerings Error

Opgave til aflevering: "generalization_error.ipynb", som viser noget om alle
koncepter, model kapacitet, under- og overfitting, og generaliseringsevne
samtidigt.

Svaret på hvordan man vælger den optimale model kapacitet ligger i
fortolkningen af Error-Capacity plots som set på denne slide.

Lader man model kapaciteten stige og plotter man så træningsfejlen, kan man få
den til at gå mod nul, idet 'systemet' så bare lærer at følge data 100% inkl.
støj og outliers. Dvs. at dets evne til at generalisere faktisk bliver mindre,
når model kapaciteten øges ud over den optimale kapacitet.

Generaliseringsfejlen, dvs. den fejl man begår på data som træningen ikke har
set (validation, test eller helt ny data), kan have en kurve, der har et
minimum ved en given kapacitet, dvs. den optimale model kapacitet---set ved den
røde vertikale linie i Error-Capacity plottet til højre. Denne optimale
kapacitet inddeler også plottet i de to formelle zoner, under- og
overfitting zone.

i) RMSE-Training set size plottet til venstre viser en "learning curve" som
fundet i HOLM.

iii) RMSE-Epoch plottet viser et error-epoch som siger noget om
"early-stopping".

Men jeg finder plot error-capacity plottet ii) den bedste visualiseringsform til at
bestemme optimal capacity.

For vores dummy binary classifier (til MNIST) ville vi ikke kunne lave et
Error-Capacity plot, idet kapaciteten er frosset fast til nul (capacity=0).
Herudover vil i) læringskurven være flad og iii) error-epoch plottet og vise
et system totalt uafhængig af antal epocher vi kører, den er ikke er i stand til
at lære/generalisere!

-----------------------------------------------------------------------------
END
